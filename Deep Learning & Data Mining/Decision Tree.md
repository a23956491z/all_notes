---
tags : machine-learning
---

### 決策樹
* 用於分類、回歸的監督式學習
* 從特徵推斷出**多個二分規則**
	* 中間節點(non-leaf node)：測試條件
	* 分支(branch)：條件測試的結果
	* 葉節點(leaf nodes)：最終的分類結果
![](https://i.imgur.com/coSBmAi.png)

產生決策樹：
1. 建立樹狀結構
2. 修剪樹狀結構

決策樹演算法：
* 樹結構由上而下產生，遞回方式建立
* 無法處理連續的數值：需要**離散化**
* 運作
	1. 所有訓練樣本在根節點
	2. 選取屬性，並將樣本分開
	3. 以統計性測量測試屬性
	4. 停止條件：
		* 節點的所有樣本都屬於同一個類別
		* 所有的屬性都用完了，以樣本數最多的類別作爲葉節點
		* 選取了某屬性後，分支完全沒有樣本
* 常用特徵優先
	* 資訊獲利最大化
	* 資訊獲利比最大化
	* 基尼指數最小化


### 資訊獲利
特定條件下，減少多少資訊不確定性
* 資訊獲利 = 資訊熵 - 條件熵
* 資訊熵：樣本的類別復雜度
![](https://i.imgur.com/KhkIMy8.png)
* 條件熵：特定特徵下，樣本的類別復雜度
![](https://i.imgur.com/3x37DHH.png)


資訊獲利比 = 資訊獲利 / 資訊熵

例如：水果資料集
![](https://i.imgur.com/0EoKGci.png)

買、不買的資訊熵
![](https://i.imgur.com/5TXnN7Q.png)

以漂亮、不漂亮做爲**特徵**：
* 漂亮的有3個，其中2個買，1個不買
![](https://i.imgur.com/Hb8ZeSa.png)

資訊獲利：

![](https://i.imgur.com/2jLbUd4.png)


### 基尼指數
表示資料的**不平均程度**
所有樣本類別相同，基尼指數爲0

* $D$：資料集
* $p(x_i)$：每個類別出現的幾率
![](https://i.imgur.com/Dkvmw4D.png)

例如：買、不買兩個類別
![](https://i.imgur.com/FLubxVk.png)
