---
tags : machine-learning
---

### 決策樹
* 用於分類、回歸的監督式學習
* 從特徵推斷出**多個二分規則**
	* 中間節點(non-leaf node)：測試條件
	* 分支(branch)：條件測試的結果
	* 葉節點(leaf nodes)：最終的分類結果
![](https://i.imgur.com/coSBmAi.png)

產生決策樹：
1. 建立樹狀結構
2. 修剪樹狀結構

決策樹演算法：
* 樹結構由上而下產生，遞回方式建立
* 無法處理連續的數值：需要**離散化**
* 運作
	1. 所有訓練樣本在根節點
	2. 選取屬性，並將樣本分開
	3. 以統計性測量測試屬性
	4. 停止條件：
		* 節點的所有樣本都屬於同一個類別
		* 所有的屬性都用完了，以樣本數最多的類別作爲葉節點
		* 選取了某屬性後，分支完全沒有樣本
* 常用特徵優先
	* 資訊獲利最大化
	* 資訊獲利比最大化
	* 基尼指數最小化

優點：
* 決策規則直觀，訓練快

缺點：
* 難以發覺屬性間的互相關聯
* 容易overfitting

### 資訊獲利
特定條件下，減少多少資訊不確定性
* 資訊獲利 = 資訊熵 - 條件熵
* 資訊熵：樣本的類別復雜度
![](https://i.imgur.com/KhkIMy8.png)
* 條件熵：特定特徵下，樣本的類別復雜度
![](https://i.imgur.com/3x37DHH.png)


資訊獲利比 = 資訊獲利 / 資訊熵

例如：水果資料集
![](https://i.imgur.com/0EoKGci.png)

買、不買的資訊熵
![](https://i.imgur.com/5TXnN7Q.png)

以漂亮、不漂亮做爲**特徵**：
* 漂亮的有3個，其中2個買，1個不買
![](https://i.imgur.com/Hb8ZeSa.png)

資訊獲利：
是否漂亮這個特徵，對於選擇買、不買的不確定度減少了`0.0056`
![](https://i.imgur.com/2jLbUd4.png)


### 基尼指數
表示資料的**不平均程度**
所有樣本類別相同，基尼指數爲0

* $D$：資料集
* $p(x_i)$：每個類別出現的幾率
![](https://i.imgur.com/Dkvmw4D.png)

例如：買、不買兩個類別
![](https://i.imgur.com/FLubxVk.png)

### 隨機森林
* 包含多個Decision Tree的分類器，輸出的類別以所有Decision Tree的投票結果爲準。
* 是一種集成學習（Ensemble Learning）


演算：
1. 隨機採樣出 n 個（可放回）
2. 隨機選擇 k 個特徵，以這些特徵建立Decision Tree
3. 重復 m 次，生成 m 棵Decision Tree
4. 新的資料，經過所有Decision Tree投票，最高票就是輸出類別

優點：
* 能處理高維度（多特徵）資料，且不需要選擇特徵
* 可判斷特徵的重要程度及其互相影響程度
* 可平行運算，因爲不同樹之間互相獨立
* 採樣的隨機性降低overfitting

缺點：
* 噪音較大的問題上容易overfitting
